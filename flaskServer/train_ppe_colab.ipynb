{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPE Detection Model Training on Google Colab\n",
        "\n",
        "This notebook trains a YOLOv11 model to detect Personal Protective Equipment (PPE) items.\n",
        "\n",
        "## Setup Instructions\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
        "2. **Upload dataset**: Run the upload cell below\n",
        "3. **Start training**: Run all cells sequentially\n",
        "\n",
        "## Training Time\n",
        "- **GPU (T4)**: 30-60 minutes\n",
        "- **CPU**: 1-3 hours (not recommended)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ultralytics YOLO\n",
        "!pip install ultralytics -q\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be very slow.\")\n",
        "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Dataset\n",
        "\n",
        "Upload your `ppe_dataset.zip` file. This should contain:\n",
        "- `datasets/images/train/` - Training images\n",
        "- `datasets/images/val/` - Validation images\n",
        "- `datasets/images/test/` - Test images\n",
        "- `datasets/labels/train/` - Training labels\n",
        "- `datasets/labels/val/` - Validation labels\n",
        "- `datasets/labels/test/` - Test labels\n",
        "- `datasets/data.yaml` - Dataset configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload dataset ZIP file\n",
        "print(\"üì§ Please upload your ppe_dataset.zip file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Find the uploaded ZIP file\n",
        "zip_filename = None\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        zip_filename = filename\n",
        "        break\n",
        "\n",
        "if zip_filename is None:\n",
        "    raise ValueError(\"No ZIP file found! Please upload ppe_dataset.zip\")\n",
        "\n",
        "print(f\"‚úÖ Found ZIP file: {zip_filename}\")\n",
        "\n",
        "# Extract dataset\n",
        "print(\"\\nüì¶ Extracting dataset...\")\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "print(\"‚úÖ Dataset extracted successfully!\")\n",
        "\n",
        "# Verify dataset structure\n",
        "dataset_path = 'datasets'\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"\\nüìÅ Dataset structure:\")\n",
        "    print(f\"   - Images train: {len(os.listdir(os.path.join(dataset_path, 'images', 'train'))) if os.path.exists(os.path.join(dataset_path, 'images', 'train')) else 0} files\")\n",
        "    print(f\"   - Images val: {len(os.listdir(os.path.join(dataset_path, 'images', 'val'))) if os.path.exists(os.path.join(dataset_path, 'images', 'val')) else 0} files\")\n",
        "    print(f\"   - Images test: {len(os.listdir(os.path.join(dataset_path, 'images', 'test'))) if os.path.exists(os.path.join(dataset_path, 'images', 'test')) else 0} files\")\n",
        "    print(f\"   - Labels train: {len(os.listdir(os.path.join(dataset_path, 'labels', 'train'))) if os.path.exists(os.path.join(dataset_path, 'labels', 'train')) else 0} files\")\n",
        "    print(f\"   - Labels val: {len(os.listdir(os.path.join(dataset_path, 'labels', 'val'))) if os.path.exists(os.path.join(dataset_path, 'labels', 'val')) else 0} files\")\n",
        "    print(f\"   - data.yaml: {'‚úÖ' if os.path.exists(os.path.join(dataset_path, 'data.yaml')) else '‚ùå'}\")\n",
        "else:\n",
        "    raise ValueError(f\"Dataset folder '{dataset_path}' not found after extraction!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Update Dataset Path (if needed)\n",
        "\n",
        "Update the `data.yaml` file to use absolute paths for Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Read current data.yaml\n",
        "yaml_path = 'datasets/data.yaml'\n",
        "with open(yaml_path, 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "# Update path to current directory\n",
        "current_dir = os.path.abspath('.')\n",
        "data['path'] = os.path.join(current_dir, 'datasets')\n",
        "\n",
        "# Save updated yaml\n",
        "with open(yaml_path, 'w') as f:\n",
        "    yaml.dump(data, f, default_flow_style=False)\n",
        "\n",
        "print(f\"‚úÖ Updated data.yaml:\")\n",
        "print(f\"   Path: {data['path']}\")\n",
        "print(f\"   Train: {data['train']}\")\n",
        "print(f\"   Val: {data['val']}\")\n",
        "print(f\"   Test: {data['test']}\")\n",
        "print(f\"   Classes: {len(data['names'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Start Training\n",
        "\n",
        "This will train the **YOLOv12** model on your PPE dataset.\n",
        "\n",
        "**Training Parameters:**\n",
        "- Model: YOLOv12n (nano - latest, better accuracy)\n",
        "- Epochs: 200 (extended training for maximum accuracy)\n",
        "- Batch size: 16 (optimized for stability)\n",
        "- Image size: 640x640\n",
        "- Early stopping: 40 epochs patience (stops automatically if no improvement)\n",
        "- Device: GPU (auto-detected)\n",
        "\n",
        "**Note:** Training may finish early if the model stops improving (early stopping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training YOLOv12 on Construction-PPE Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\nüñ•Ô∏è  Device: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  WARNING: No GPU! Training will be very slow.\")\n",
        "\n",
        "# Load pre-trained YOLOv12 nano model (latest, better accuracy)\n",
        "print(\"\\n[1/3] Loading YOLOv12n model...\")\n",
        "print(\"   YOLOv12: 1.2% better mAP than YOLOv11, same speed\")\n",
        "model = YOLO(\"yolo12n.pt\")\n",
        "print(\"‚úÖ Model loaded\\n\")\n",
        "\n",
        "# Dataset path\n",
        "dataset_yaml = 'datasets/data.yaml'\n",
        "print(f\"üìÅ Dataset: {dataset_yaml}\")\n",
        "\n",
        "# Training parameters\n",
        "print(\"\\n[2/3] Starting training...\")\n",
        "print(\"   - Epochs: 200 (extended training for maximum accuracy)\")\n",
        "print(\"   - Batch size: 16 (optimized for stability)\")\n",
        "print(\"   - Image size: 640\")\n",
        "print(\"   - Early stopping patience: 40 epochs (stops if no improvement)\")\n",
        "print(\"\\n‚è≥ Training will take 1.5-3 hours on GPU...\")\n",
        "print(\"   (May finish earlier if model plateaus - early stopping)\")\n",
        "print(\"   (You can monitor progress below)\\n\")\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    results = model.train(\n",
        "        data=dataset_yaml,\n",
        "        epochs=200,  # Extended training - early stopping prevents overfitting\n",
        "        imgsz=640,\n",
        "        batch=16,  # Smaller batch for better stability\n",
        "        device=device,\n",
        "        project=\"runs/detect\",\n",
        "        name=\"ppe_detection\",\n",
        "        exist_ok=True,\n",
        "        patience=40,  # Early stopping - stops if no improvement for 40 epochs\n",
        "        save=True,\n",
        "        plots=True,\n",
        "        workers=4,\n",
        "        cache=True,  # Cache images in RAM\n",
        "        amp=True,  # Mixed precision for faster training\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[3/3] Training Complete!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Show results\n",
        "    best_model_path = \"runs/detect/ppe_detection/weights/best.pt\"\n",
        "    if os.path.exists(best_model_path):\n",
        "        file_size = os.path.getsize(best_model_path) / (1024 * 1024)  # MB\n",
        "        print(f\"\\n‚úÖ Model saved to: {best_model_path}\")\n",
        "        print(f\"‚úÖ Model size: {file_size:.1f} MB\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  Model file not found!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Download Trained Model\n",
        "\n",
        "Download the trained `best.pt` model to your Mac.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "best_model_path = \"runs/detect/ppe_detection/weights/best.pt\"\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"üì• Downloading trained model...\")\n",
        "    print(f\"   File: {best_model_path}\")\n",
        "    \n",
        "    file_size = os.path.getsize(best_model_path) / (1024 * 1024)  # MB\n",
        "    print(f\"   Size: {file_size:.1f} MB\")\n",
        "    \n",
        "    # Download file\n",
        "    files.download(best_model_path)\n",
        "    \n",
        "    print(\"\\n‚úÖ Download started!\")\n",
        "    print(\"\\nüìã Next steps:\")\n",
        "    print(\"   1. Save the downloaded 'best.pt' file\")\n",
        "    print(\"   2. Copy it to your Mac:\")\n",
        "    print(\"      InsolareSafetySystem/flaskServer/runs/detect/ppe_detection/weights/\")\n",
        "    print(\"   3. Replace the existing best.pt file\")\n",
        "    print(\"   4. Restart your Flask server\")\n",
        "else:\n",
        "    print(\"‚ùå Model file not found!\")\n",
        "    print(\"   Training may not have completed successfully.\")\n",
        "    print(\"   Check the training output above for errors.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
